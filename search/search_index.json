{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Ripper Docs","text":"<p>Welcome! If you're getting started, check out:</p> <ul> <li>Beginners: a plug-and-play demo with no downloads</li> <li>Quickstart: full pipeline with extraction \u2192 transplant \u2192 validation</li> <li>Architecture: a visual overview of modules and data flow</li> </ul> <p>Use the navigation on the left to explore guides and API reference.</p>"},{"location":"api/","title":"API Reference","text":"<p>Documentation for the LLM Ripper API will be available soon.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li>extraction: Model component extraction utilities</li> <li>validation: Model validation and testing utilities  </li> <li>analysis: Analysis and reporting tools</li> <li>transplant: Component transplantation utilities</li> </ul>"},{"location":"api/#utils","title":"Utils","text":"<ul> <li>run: Runtime context and execution utilities</li> <li>model_loader: Model loading and configuration utilities</li> </ul> <p>Auto-generated API documentation coming soon.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>LLM Ripper is organized as a modular framework around extraction, analysis, transplantation, and validation of knowledge from Transformer-based language models.</p>"},{"location":"architecture/#high-level-flow","title":"High-level flow","text":"<pre><code>flowchart LR\n  A[Models] --&gt;|activation capture| B[Core: activation_capture]\n  B --&gt; C[Core: analysis]\n  C --&gt; D[Knowledge Bank]\n  D --&gt; E[Core: transplant]\n  E --&gt; F[Core: validation]\n  C --&gt; G[Causal tracing]\n  E --&gt; H[Studio viewer]\n</code></pre>"},{"location":"architecture/#packages","title":"Packages","text":"<ul> <li>llm_ripper.core</li> <li>activation_capture.py: capture activations to HDF5/NP arrays</li> <li>analysis.py: component/feature analysis, head catalogs</li> <li>extraction.py: build Knowledge Bank assets</li> <li>transplant.py: modular transplant strategies</li> <li>validation.py: quantitative checks, diagnostics</li> <li>llm_ripper.utils</li> <li>model_loader.py: safe/controlled model loading</li> <li>run.py: run directory and artifact writers</li> <li>config.py: configuration helpers</li> <li>llm_ripper.causal: tracing utilities</li> <li>llm_ripper.interop: adapters and merging</li> <li>llm_ripper.safety: provenance and reports</li> <li>llm_ripper.studio: lightweight static viewer</li> </ul>"},{"location":"architecture/#artifacts-structure-runcontext","title":"Artifacts structure (RunContext)","text":"<p>Runs are created under runs// with subdirectories: - knowledge_bank, activations, analysis, transplants, validation, causal, traces, counterfactuals, uq, catalog, provenance, reports <p>See docs/api.md for API reference and README for quickstart.</p>"},{"location":"beginners/","title":"Beginner's Guide (Start Here)","text":"<p>Welcome! This guide is for developers new to Python ML projects. You'll get a working demo in minutes, no GPU required.</p>"},{"location":"beginners/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8\u20133.11</li> <li>Git (optional)</li> </ul>"},{"location":"beginners/#1-set-up-your-environment","title":"1) Set up your environment","text":"<pre><code># inside your project folder\npython -m venv .venv\n# Windows: .venv\\\\Scripts\\\\activate\n# macOS/Linux:\nsource .venv/bin/activate\n\npip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -e .\npre-commit install\n</code></pre>"},{"location":"beginners/#2-run-the-beginner-demo-no-downloads","title":"2) Run the beginner demo (no downloads)","text":"<p>Option A (Makefile): This creates a sample run folder with the files the Studio viewer expects.</p> <pre><code>make beginner\n</code></pre> <p>Option B (CLI only):</p> <pre><code>llm-ripper quickstart --open\n</code></pre> <p>The command will: - Generate a run under <code>runs/&lt;timestamp&gt;/</code> with demo JSON files - Launch the Studio viewer at http://localhost:8000</p> <p>If the page shows empty panels or error messages, that's okay \u2014 you can still explore the layout and JSON files.</p>"},{"location":"beginners/#3-next-steps","title":"3) Next steps","text":"<ul> <li>Try the offline smoke test: <code>make smoke-offline</code></li> <li>Explore CLI help: <code>python -m llm_ripper.cli --help</code></li> <li>Read the Quickstart for full pipeline steps</li> </ul>"},{"location":"beginners/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If <code>mkdocs</code> or <code>ruff</code> commands are missing, install dev deps: <code>pip install -r requirements-dev.txt</code></li> <li>If ports are busy, change the Studio port: <code>make studio PORT=8001</code></li> </ul> <p>You're set! As you gain confidence, switch from the demo to actual models and data.</p>"},{"location":"maintainers/","title":"Maintainers Guide","text":"<p>This guide lists the steps to enable CI/CD and publishing for this repository.</p>"},{"location":"maintainers/#1-enable-github-pages-docs","title":"1) Enable GitHub Pages (Docs)","text":"<ul> <li>Go to Settings \u2192 Pages \u2192 Build and deployment \u2192 Source: GitHub Actions</li> <li>After pushing to <code>main</code>, the <code>Docs Deploy</code> workflow publishes to:   https://qrv0.github.io/LLM-Ripper</li> </ul>"},{"location":"maintainers/#2-codecov-coverage","title":"2) Codecov (coverage)","text":"<ul> <li>Create a Codecov account and obtain a repository token</li> <li>In GitHub: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret</li> <li>Name: <code>CODECOV_TOKEN</code></li> <li>Value: your token</li> <li>The CI workflow already uploads coverage (XML) via codecov-action</li> </ul>"},{"location":"maintainers/#3-pypi-releases","title":"3) PyPI releases","text":"<ul> <li>Create an API token on PyPI</li> <li>In GitHub: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret</li> <li>Name: <code>PYPI_API_TOKEN</code></li> <li>Value: <code>&lt;pypi-AgEN...&gt;</code></li> <li>Tag a release to publish: <pre><code>git tag v1.0.0\ngit push origin v1.0.0\n</code></pre></li> </ul>"},{"location":"maintainers/#4-branch-protection-for-main","title":"4) Branch protection for main","text":"<ul> <li>Settings \u2192 Branches \u2192 Add rule for <code>main</code></li> <li>Require pull request before merging</li> <li>Require status checks to pass (CI)</li> </ul>"},{"location":"maintainers/#5-local-quality-checks","title":"5) Local quality checks","text":"<ul> <li>Pre-commit: <code>pre-commit install</code></li> <li>Lint/format: <code>make lint</code> / <code>make format</code></li> <li>Tests: <code>make test</code> / <code>make test-cov</code></li> <li>Docs: <code>make docs-serve</code></li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Install in editable mode and run the CLI:</p> <pre><code>pip install -e .\nllm-ripper --about\n</code></pre>"},{"location":"guides/causal_tracing/","title":"Causal Tracing","text":"<p>Causal tracing runs controlled interventions on internal components (heads/FFN) and measures impact on a task metric.</p>"},{"location":"guides/causal_tracing/#targets","title":"Targets","text":"<ul> <li>Attention projections: <code>head:&lt;layer&gt;.&lt;q|k|v|o&gt;[:head_idx]</code></li> <li>FFN parts: <code>ffn:&lt;layer&gt;.&lt;gate|up|down&gt;</code></li> </ul> <p>Examples: <code>head:0.q</code>, <code>head:12.o:3</code>, <code>ffn:7.up</code>.</p>"},{"location":"guides/causal_tracing/#interventions","title":"Interventions","text":"<ul> <li><code>zero</code>: zero out the selected subspace</li> <li><code>noise</code>: add small Gaussian noise</li> <li><code>mean-patch</code>: replace activations with the mean across batch/time</li> </ul>"},{"location":"guides/causal_tracing/#metrics","title":"Metrics","text":"<ul> <li><code>nll_delta</code>: difference in language-model loss (lower is better baseline)</li> <li><code>logit_delta</code>: difference in target token logit (eos proxy)</li> </ul>"},{"location":"guides/causal_tracing/#cli","title":"CLI","text":"<pre><code>llm-ripper trace \\\n  --model &lt;path_or_hf&gt; \\\n  --targets head:12.q:0,ffn:7.up \\\n  --metric nll_delta \\\n  --intervention zero \\\n  --max-samples 64 --seed 42 --json\n</code></pre> <p>Outputs: - <code>runs/&lt;stamp&gt;/traces/traces.jsonl</code>: per-target rows <code>{target, intervention, metric, baseline, intervened, delta}</code> - <code>runs/&lt;stamp&gt;/traces/summary.json</code>: ranked by <code>delta</code> descending</p>"},{"location":"guides/end_to_end/","title":"End-to-End Pipeline","text":"<p>This guide walks through an end-to-end run using the new features:</p> <ul> <li>Extraction (embeddings) \u2192 Analysis</li> <li>Causal Tracing \u2192 Alignment</li> <li>Transplant (embeddings init) \u2192 Mechanistic Validation (offline)</li> <li>UQ + Routing Simulation</li> <li>Counterfactuals (generate/evaluate)</li> <li>Stress/Drift (PSI/KL)</li> <li>Report (JSON/MD/PDF)</li> <li>Studio (MVP)</li> </ul>"},{"location":"guides/end_to_end/#prerequisites","title":"Prerequisites","text":"<ul> <li>A local model directory or a HuggingFace model id (e.g., <code>gpt2</code>).</li> <li>Optional: baseline model id/path for drift comparisons.</li> </ul>"},{"location":"guides/end_to_end/#one-file-example","title":"One-File Example","text":"<p>Run the full pipeline example script:</p> <pre><code>python examples/run_full_pipeline.py --model gpt2 --baseline gpt2\n</code></pre> <p>Outputs are written to <code>runs/&lt;stamp&gt;/</code> in standardized folders.</p>"},{"location":"guides/end_to_end/#manual-cli-path","title":"Manual CLI Path","text":"<pre><code># 1) Extract + Analyze\nllm-ripper extract --model gpt2 --output-dir ./knowledge_bank\nllm-ripper analyze --knowledge-bank ./knowledge_bank --output-dir ./analysis\n\n# 2) Trace (impact ranking)\nllm-ripper trace --model gpt2 --targets head:0.q,ffn:0.up --metric nll_delta --intervention zero --max-samples 64\n\n# 3) Alignment\nllm-ripper bridge-align --source ./knowledge_bank --target gpt2 --out ./transplants/W_align\n\n# 4) Transplant (embeddings init)\nllm-ripper transplant --source ./knowledge_bank --target gpt2 --output-dir ./transplants --strategy embedding_init --source-component embeddings\n\n# 5) Mechanistic validation (offline)\nllm-ripper validate --model ./transplants --mechanistic --output-dir ./validation\n\n# 6) UQ + routing\nllm-ripper uq --model ./transplants --samples 10 --max-texts 64\nllm-ripper route-sim --metrics runs/&lt;stamp&gt;/uq/metrics.jsonl --tau 0.7\n\n# 7) Counterfactuals\nllm-ripper cfgen --task agreement --n 2000 --out ./counterfactuals/pairs.jsonl\nllm-ripper cfeval --model ./transplants --pairs ./counterfactuals/pairs.jsonl --out ./counterfactuals/results.jsonl\n\n# 8) Stress &amp; Drift\nllm-ripper stress --model ./transplants --baseline gpt2 --out ./reports\n\n# 9) Report\nllm-ripper report --ideal --out ./reports --from ./runs/&lt;stamp&gt;\n\n# 10) Studio\nllm-ripper studio --root ./runs/&lt;stamp&gt; --port 8000\n</code></pre>"},{"location":"guides/end_to_end/#tips","title":"Tips","text":"<ul> <li>Prefer local paths when running offline; pass <code>--offline</code> to subcommands that might download datasets.</li> <li><code>merge --global spec.yaml --micro</code> supports global weight average and microtransplants in one step.</li> <li><code>adapters --import &lt;lora&gt;</code> adds an extra adapter to a layer and <code>--fuse</code> attaches a fusion gate over all adapters.</li> </ul>"},{"location":"guides/interop/","title":"Interoperability (Merge, Adapters, Tokenizers)","text":""},{"location":"guides/interop/#global-merge","title":"Global Merge","text":"<p>Average weights across models listed in a spec.</p> <pre><code>llm-ripper merge --global examples/merge_spec.yaml --out ./transplants/merged\nllm-ripper merge --global examples/merge_spec.yaml --micro --out ./transplants/merged  # + microtransplants\n</code></pre> <p>Spec (YAML/JSON):</p> <pre><code>models:\n  - ./models/modelA\n  - ./models/modelB\nbase_dir: ./models/modelA\nmicro:\n  - knowledge_bank: ./knowledge_bank\n    source_component: embeddings\n    target_layer: 0\n    strategy: embedding_init\n</code></pre>"},{"location":"guides/interop/#adapters","title":"Adapters","text":"<p>Import a simple LoRA file and inject as an additional adapter; optionally attach a fusion gate.</p> <pre><code>llm-ripper adapters --model ./transplanted/model --import lora.safetensors --layer 3 --fuse\n</code></pre>"},{"location":"guides/interop/#tokenizer-alignment","title":"Tokenizer Alignment","text":"<p>Emit token overlap and id mapping.</p> <pre><code>llm-ripper tokenize-align --source gpt2 --target distilgpt2 --out ./catalog/tok_align.json\n</code></pre>"},{"location":"guides/safety_reporting/","title":"Safety, Provenance, and Reporting","text":""},{"location":"guides/safety_reporting/#provenance-scan","title":"Provenance Scan","text":"<p>Verify transplanted directories for required artifacts and license hints.</p> <pre><code>llm-ripper provenance --scan ./transplanted --fail-on-violation --json\n</code></pre> <p>Checks: transplant metadata, presence of model/tokenizer files, license files, and SHA-256 hashes of key files.</p>"},{"location":"guides/safety_reporting/#stress-ood-drift","title":"Stress &amp; OOD Drift","text":"<p>Compare distributions between a candidate model and a baseline using entropy histograms (PSI) and mean softmax KL.</p> <pre><code>llm-ripper stress --model ./transplanted --baseline gpt2 --out ./reports\n</code></pre> <p>Output: <code>reports/stress_drift.json</code> with <code>psi_entropy</code>, <code>kl_model_vs_baseline</code>, <code>kl_baseline_vs_model</code>.</p>"},{"location":"guides/safety_reporting/#reporting","title":"Reporting","text":"<p>Aggregate results (validation, UQ) into JSON/MD/PDF.</p> <pre><code>llm-ripper report --ideal --out ./reports --from ./runs/&lt;stamp&gt;\n</code></pre> <p>Outputs: - <code>reports/report.json</code> - <code>reports/report.md</code> - <code>reports/report.pdf</code></p>"},{"location":"guides/transplant_strategies/","title":"Transplant Strategies","text":"<ul> <li>Embedding initialization</li> <li>Module injection with bridges</li> <li>Adapter Fusion (multi-adapter layers)</li> </ul>"},{"location":"guides/transplant_strategies/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n  A[Donor Model] --&gt;|extract| KB[Knowledge Bank]\n  A --&gt;|capture| ACT[Activations (HDF5)]\n  KB --&gt;|analyze| ANALYSIS[Analysis &amp; Catalog]\n\n  subgraph Interop &amp; Bridges\n    MERGE[merge --global]:::op\n    ADAPT[adapters --import/--fuse]:::op\n    TOK[tokenize-align]:::op\n    ALIGN[bridge-align]:::op\n    BTRAIN[bridge-train --mixture]:::op\n  end\n\n  ANALYSIS --&gt; ALIGN\n  KB --&gt; ALIGN\n  MERGE --&gt; MRGD[Merged Model]\n  MRGD --&gt; TRANS[Transplanted Model]\n  KB --&gt;|transplant| TRANS\n  ADAPT --&gt; TRANS\n\n  classDef op fill:#eef,stroke:#446,stroke-width:1px;\n</code></pre>"},{"location":"guides/transplant_strategies/#injection-flow-module-bridges-gate","title":"Injection Flow (Module + Bridges + Gate)","text":"<pre><code>sequenceDiagram\n  autonumber\n  participant H as Hidden state h\n  participant IB as InputBridge\n  participant DM as DonorModule\n  participant OB as OutputBridge\n  participant BASE as BaseSubmodule\n  participant G as FusionGate\n  participant Y as Layer Output\n\n  H-&gt;&gt;IB: project if dims mismatch\n  IB--&gt;&gt;DM: h'\n  DM-&gt;&gt;OB: donor forward\n  OB--&gt;&gt;G: transplanted_out\n  H-&gt;&gt;BASE: base forward\n  BASE--&gt;&gt;G: base_out\n  G--&gt;&gt;Y: fused_out (learned gate)\n</code></pre>"},{"location":"guides/transplant_strategies/#notes","title":"Notes","text":"<ul> <li>Embedding Initialization: If donor/target dims differ, an input bridge aligns donor embeddings to target space and initializes target embeddings accordingly.</li> <li>Module Injection: Donor module wrapped with input/output bridges; injected on target layer; fusion gate combines base and transplanted paths.</li> <li>Adapter Fusion: Multiple transplanted modules on a layer; a gate (or attention-like combiner) fuses outputs from adapters into the base path.</li> </ul>"},{"location":"guides/transplant_strategies/#strategy-selection-rules-of-thumb","title":"Strategy Selection (Rules of Thumb)","text":"<ul> <li>Embedding Initialization:</li> <li>Use when transferring vocabulary/semantic priors and alignment error is small (after <code>bridge-align</code>, cosine\u2191, MSE\u2193).</li> <li>Good first step when donor/target are architecturally similar and you want a quick measurable gain.</li> <li>Module Injection + Bridges:</li> <li>Use when causal tracing highlights specific heads/FFN parts with strong \u0394-impact.</li> <li>Bridges reconcile dim mismatches; prefer for surgical, interpretable transplants.</li> <li>Adapter Fusion (Multi-Adapter Layers):</li> <li>Use when multiple candidates on the same layer show complementary effects or reduce variance across tasks.</li> <li>Attach a fusion gate and, if needed, fine-tune gate alphas (kept lightweight).</li> <li>Mixture-of-Bridges Training:</li> <li>Use to stabilize performance on heterogeneous tasks by learning k specialists with a small gate.</li> <li>Train only adapters and gate offline/self-supervised to match base layer outputs (MSE objective).</li> <li>Bridge-Align (Procrustes) Pre-Step:</li> <li>Run before injection to reduce geometric distortion donor\u2192target; improves warm-start of bridges.</li> <li>Evidence Gathering:</li> <li>Run <code>trace</code> first to rank candidates; confirm with counterfactuals (<code>cfgen/cfeval</code>) on minimal pairs.</li> <li>Cross-check with UQ (<code>uq</code>) to ensure no-regress under uncertainty; route to baseline when UQ &gt; \u03c4 (<code>route-sim</code>).</li> </ul>"},{"location":"guides/transplant_strategies/#strategy-decision-flow","title":"Strategy Decision Flow","text":"<pre><code>flowchart TD\n  S([Start]) --&gt; A{Aligned donor\u2192target?\\n(cos\u2191, MSE\u2193)}\n  A -- No --&gt; BA[Run bridge-align]\\n--&gt; A\n  A -- Yes --&gt; T{High \u0394-impact targets?\\n(trace)}\n  T -- Yes --&gt; INJ[Module injection + bridges]\n  T -- No --&gt; V{Need vocab/semantics transfer?}\n  V -- Yes --&gt; EMB[Embedding initialization]\n  V -- No --&gt; H{Heterogeneous tasks?}\n  H -- Yes --&gt; MOB[Mixture-of-bridges training]\n  H -- No --&gt; L{Multiple candidates per layer?}\n  L -- Yes --&gt; AF[Adapter Fusion]\n  L -- No --&gt; VAL[Validate + UQ]\n\n  INJ --&gt; VAL\n  EMB --&gt; VAL\n  MOB --&gt; VAL\n  AF  --&gt; VAL\n\n  VAL --&gt; CF[Counterfactuals (cfgen/cfeval)]\n  VAL --&gt; UQ[UQ + route-sim (\u03c4)]\n  UQ  --&gt; REP[Report + Studio]\n  CF  --&gt; REP\n</code></pre>"},{"location":"guides/uq_routing/","title":"Uncertainty &amp; Routing","text":"<p>The UQ module uses MC\u2011Dropout to estimate predictive uncertainty and calibration, and can simulate routing fallbacks.</p>"},{"location":"guides/uq_routing/#metrics","title":"Metrics","text":"<ul> <li><code>predictive_entropy</code>: entropy of the mean posterior</li> <li><code>mutual_info</code> (BALD): epistemic uncertainty proxy</li> <li><code>ECE</code>: self-consistency calibration proxy (confidence vs agreement)</li> </ul>"},{"location":"guides/uq_routing/#cli","title":"CLI","text":"<pre><code>llm-ripper uq --model &lt;path_or_hf&gt; --samples 20 --max-texts 128\nllm-ripper route-sim --metrics runs/&lt;stamp&gt;/uq/metrics.jsonl --tau 0.7\n</code></pre> <p>Outputs: - <code>runs/&lt;stamp&gt;/uq/metrics.jsonl</code>: per-example metrics - <code>runs/&lt;stamp&gt;/uq/summary.json</code>: aggregate metrics - routing sim prints <code>{routed, routed_frac}</code> for threshold \u03c4.</p>"},{"location":"guides/validation/","title":"Validation","text":"<p>Overview of intrinsic and extrinsic validation protocols, metrics, and summaries.</p>"}]}